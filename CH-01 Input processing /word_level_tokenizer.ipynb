{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4638c804",
   "metadata": {},
   "source": [
    " Implement a Basic Word-Level Tokenizer\n",
    "âœ… Task\n",
    "Create a WordTokenizer class with:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "class WordTokenizer:\n",
    "    def fit(self, texts): ...\n",
    "    def tokenize(self, text): ...\n",
    "    def convert_tokens_to_ids(self, tokens): ...\n",
    "    def convert_ids_to_tokens(self, ids): ...\n",
    "ðŸ’¡ Bonus\n",
    "Add a special token for <UNK> and <PAD>, and support fixed vocab size (e.g., top 10,000 tokens only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5392ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, vocab_size=10000, min_freq=1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.special_tokens = ['<PAD>', '<UNK>']\n",
    "\n",
    "    def fit(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = tokenize(text)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # Filter tokens\n",
    "        filtered = [tok for tok, freq in counter.items() if freq >= self.min_freq]\n",
    "        vocab = self.special_tokens + filtered[:self.vocab_size - len(self.special_tokens)]\n",
    "\n",
    "        self.token_to_id = {tok: idx for idx, tok in enumerate(vocab)}\n",
    "        self.id_to_token = {idx: tok for tok, idx in self.token_to_id.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return tokenize(text)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_id.get(tok, self.token_to_id['<UNK>']) for tok in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.id_to_token.get(i, '<UNK>') for i in ids]\n",
    "\n",
    "# Example\n",
    "wtok = WordTokenizer()\n",
    "wtok.fit(corpus)\n",
    "tokens = wtok.tokenize(\"Deep learning is fun\")\n",
    "ids = wtok.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n",
    "print(\"Recovered:\", wtok.convert_ids_to_tokens(ids))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
